{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUbvariant COmbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "def get_filterd_dataset(product_name):\n",
    "\n",
    "    new_df = pd.read_csv('data/master/master_historical_till_2023_10_17.csv')\n",
    "    # new_df = new_df.dropna(how='all')\n",
    "\n",
    "    filtered = new_df[new_df['product_name']==product_name]\n",
    "    filtered['order_submited_datetime'] = pd.to_datetime(filtered['order_submited_datetime'])\n",
    "    return filtered\n",
    "\n",
    "def add_inflation(new_df):\n",
    "    values = [\n",
    "        3.68, 3.37, 3.14, 3.13, 3.16, 3.2, 3.12, 3.1, 3.23, 3.33, 3.27, 3.18, 3.15, 3.01, 3.21, 3.25, 3.31, 3.43, 3.79, 3.75,\n",
    "        3.82, 3.86, 3.84, 3.8, 3.62, 3.72, 3.86, 3.51, 2.85, 2.19, 1.97, 1.88, 1.97, 1.75, 1.49, 1.61, 1.6, 1.56, 1.51, 1.95,\n",
    "        3.3, 3.63, 3.97, 4.44, 4.51, 4.58, 5.26, 5.62, 6.94, 8.01, 8.53, 9.23, 9.07, 9.67, 10.21, 10.84, 11.44, 12.22, 12.53,\n",
    "        13.12, 13.25, 13.28, 13.34, 12.82, 12.36, 12.13, 11.78, 11.43,10.79, 10.79\n",
    "    ]\n",
    "\n",
    "    # Create a list of dates from Jan 2018 to Aug 2023\n",
    "    date_range = pd.date_range(start='2018-01-01', end='2023-10-30', freq='D')\n",
    "\n",
    "    # Create a DataFrame\n",
    "\n",
    "    inflation = []\n",
    "    df = pd.DataFrame()\n",
    "    df['date'] = date_range\n",
    "    i = 0\n",
    "    prev_month = df['date'][0].month\n",
    "    # Populate the 'Price' column with values based on the corresponding month's value\n",
    "    for date in df['date']:\n",
    "        month = date.month\n",
    "        if prev_month != month:\n",
    "            prev_month = month\n",
    "            i+=1\n",
    "        inflation.append(values[i])\n",
    "        # df.at[date, 'Price'] = value\n",
    "\n",
    "    # Print the resulting DataFrame\n",
    "    df['inflation'] = inflation\n",
    "    # print(df)\n",
    "    # df = df.set_index('date')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    new_df['date'] = pd.to_datetime(new_df['date'])\n",
    "    # filtered2 = pd.merge(filtered2, df, how='left', left_index=True, right_index=True)\n",
    "    merged = pd.merge(new_df, df, on='date', how='left')\n",
    "    return merged\n",
    "\n",
    "def generate_datafrane_for_profit2(date, price):\n",
    "    test_date = pd.to_datetime(date)\n",
    "    dates = [test_date.date()] * 24\n",
    "    prices = [price] * 24\n",
    "    hours = list(range(24))\n",
    "    \n",
    " \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'datetime': dates, 'hour': hours, 'net_price': prices})\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    df['day_of_week_n'] = df['datetime'].dt.day_of_week\n",
    "\n",
    "    for day_num in range(0, 7):\n",
    "        day_col_name = f'net_price_day_{day_num}'\n",
    "        df[day_col_name] = df.apply(lambda row: row['net_price'] if row['day_of_week_n'] == day_num else 0, axis=1)\n",
    "    # merged_Df = add_inflation(df)[columns[:-1]]\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    merged_Df = df\n",
    "    \n",
    "    \n",
    "    return merged_Df.sort_values(['date','hour'])\n",
    "\n",
    "def mode(x):\n",
    "    return stats.mode(x)[0][0] \n",
    "\n",
    "def add_to_Historic_Dataframe(new_data,date):\n",
    "  # new_data = pd.read_csv('data/Frubana_2023_10_09.csv')\n",
    "  \n",
    "  historical =  pd.read_csv('historic_pilot_info_catalan.csv')\n",
    "  colombia_tz = pytz.timezone('America/Bogota')\n",
    "    # Convert the 'order_submited_datetime' column to Colombia timezone\n",
    "  historical['order_submited_datetime'] = pd.to_datetime(historical['order_submited_datetime'])\n",
    "  historical['order_submited_datetime'] = historical['order_submited_datetime'].dt.tz_convert(colombia_tz)\n",
    "\n",
    "  new_data['order_submited_datetime'] = pd.to_datetime(new_data['order_submited_datetime'])\n",
    "  new_data['order_submited_datetime'] = new_data['order_submited_datetime'].dt.tz_localize(pytz.utc).dt.tz_convert(colombia_tz)\n",
    "  new_data['order_submited_date'] = new_data['order_submited_datetime'].dt.date.astype(str)\n",
    "  \n",
    "  new_data = new_data[new_data['order_submited_date']>date]\n",
    "\n",
    "  common_columns = new_data.columns.intersection(historical.columns)\n",
    "  merged_df = pd.concat([historical[common_columns], new_data[common_columns]])\n",
    "\n",
    "  merged_df.sort_values('order_submited_datetime').to_csv(f'data/created/Catalan_{date}.csv')\n",
    "\n",
    "  return merged_df.sort_values('order_submited_datetime')\n",
    "\n",
    "\n",
    "def get_papa_grande():\n",
    "    df = get_filterd_dataset('Champiñón Blanco Paris 500 gr Bandeja')\n",
    "    colombia_tz = pytz.timezone('America/Bogota')\n",
    "\n",
    "    # Convert the 'order_submited_datetime' column to Colombia timezone\n",
    "    if df['order_submited_datetime'].dt.tz is None:\n",
    "    # If the datetime is naive, localize it\n",
    "        df['order_submited_datetime'] = df['order_submited_datetime'].dt.tz_localize(pytz.utc).dt.tz_convert(colombia_tz)\n",
    "    else:\n",
    "        # If the datetime is already timezone-aware, just convert it\n",
    "        df['order_submited_datetime'] = df['order_submited_datetime'].dt.tz_convert(colombia_tz)\n",
    "\n",
    "    df['order_submited_datetime'] = pd.to_datetime(df['order_submited_datetime'])\n",
    "    df['hour'] = df['order_submited_datetime'].dt.hour\n",
    "    df['date'] = df['order_submited_datetime'].dt.date\n",
    "    df['month'] = df['order_submited_datetime'].dt.month\n",
    "    # df['day_of_week_n'] = df['order_submited_datetime'].dt.day_of_week\n",
    "    df['day_of_month'] = df['order_submited_datetime'].dt.day\n",
    "    df['year'] = df['order_submited_datetime'].dt.year\n",
    "    agg_df = df.groupby(['date', 'hour']).agg({\n",
    "    'net_price': mode,\n",
    "    'quantity': 'sum',\n",
    "    'cost': 'first',\n",
    "    'avg_bench': 'first',\n",
    "    'product_id': 'first',\n",
    "    'product_name': 'first',\n",
    "    'gross_price': 'first',\n",
    "    # 'day_of_week_n': 'first',\n",
    "    'day_of_month': 'first',\n",
    "    # 'date': 'first'\n",
    "    'year':'first',\n",
    "    'month': 'first'\n",
    "\n",
    "    }).reset_index()\n",
    "\n",
    "    min_date = pd.to_datetime(agg_df['date']).min()\n",
    "    max_date = pd.to_datetime(agg_df['date']).max()\n",
    "\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    hour_range = list(range(24))\n",
    "\n",
    "    full_df = pd.DataFrame([(date, hour) for date in date_range for hour in hour_range], columns=['date', 'hour'])\n",
    "    full_df['date'] = full_df['date'].dt.date.astype(str)  # Convert datetime to string to match df's 'date' format\n",
    "    temp = agg_df[['date','hour']]\n",
    "    temp_unique = temp.drop_duplicates()\n",
    "    temp_unique['date'] = pd.to_datetime(temp_unique['date'])\n",
    "    temp_unique['date'] = temp_unique['date'].dt.date.astype(str)\n",
    "    # Identify missing rows\n",
    "    merged_df = pd.merge(full_df, temp_unique, on=['date', 'hour'], how='left', indicator=True)\n",
    "    missing_rows = merged_df[merged_df['_merge'] == 'left_only'][['date', 'hour']]\n",
    "    # missing_rows.head(23)\n",
    "    agg_df['date'] = pd.to_datetime(agg_df['date'])\n",
    "    agg_df['date'] = agg_df['date'].dt.date.astype(str)\n",
    "    merged_df.to_csv('temp.csv')\n",
    "    \n",
    "    merged_df = pd.merge(agg_df, missing_rows, on=['date', 'hour'], how='outer')\n",
    "    merged_df.to_csv('temp3.csv')\n",
    "    \n",
    "    new_date = merged_df.iloc[-1]['date']\n",
    "    new_price = merged_df.iloc[-1]['net_price']\n",
    "\n",
    "\n",
    "    date_format = '%Y-%m-%d'\n",
    "    date_DATE = datetime.strptime(new_date, date_format)\n",
    "    # Add one day to the date\n",
    "    next_date = date_DATE + timedelta(days=1)\n",
    "    next_date_str = next_date.strftime(date_format)\n",
    "    print('next date',next_date_str)\n",
    "    test_vector_copy = generate_datafrane_for_profit2(next_date_str,new_price )\n",
    "    print('new_test_vecotr',test_vector_copy.iloc[-1]['date'])\n",
    "    merged_df = merged_df.sort_values(['date','hour'])\n",
    "    merged_df = pd.concat([ merged_df, test_vector_copy])\n",
    "\n",
    "    test_vector_copy = generate_datafrane_for_profit2('2023-10-19',new_price )\n",
    "    print('new_test_vecotr',test_vector_copy.iloc[-1]['date'])\n",
    "    merged_df = merged_df.sort_values(['date','hour'])\n",
    "    merged_df = pd.concat([ merged_df, test_vector_copy])\n",
    "    print('new_row',merged_df.iloc[-1]['date'])\n",
    "\n",
    "    merged_df.to_csv('temp4.csv')\n",
    "    \n",
    "    print('new_row',merged_df.iloc[-1]['date'])\n",
    "\n",
    "\n",
    "    cols_to_fill = [col for col in merged_df.columns if col not in ['quantity']]\n",
    "    merged_df = merged_df.sort_values(['date','hour'])\n",
    "    \n",
    "    merged_df['net_price'] = merged_df['net_price'].fillna(method='ffill')\n",
    "    merged_df['quantity'].fillna(0, inplace=True)\n",
    "    merged_df.to_csv('temp5.csv')\n",
    "    \n",
    "    merged_df['net_price_FP'] = merged_df['net_price']\n",
    "    merged_df['quantity_FP'] = merged_df['quantity']\n",
    "    merged_df['net_price_FP_lag48'] = merged_df['net_price_FP'].shift(48)\n",
    "    merged_df['net_price_FP_lag49'] = merged_df['net_price_FP'].shift(49)\n",
    "    merged_df['quantity_FP_lag48'] = merged_df['quantity_FP'].shift(48)\n",
    "    merged_df['quantity_FP_lag49'] = merged_df['quantity_FP'].shift(49)\n",
    "    \n",
    "    # if not comp\n",
    "    daily_quantity_papagrande = merged_df.groupby('date').agg({'quantity': 'sum','net_price': mode}).reset_index()\n",
    "    daily_quantity_papagrande.to_csv('temp1.csv')\n",
    "    result = seasonal_decompose(daily_quantity_papagrande['quantity'], model='additive', period=7)\n",
    "    daily_quantity_papagrande['FP_residual_daily'] = result.resid\n",
    "    daily_quantity_papagrande['FP_trend_daily'] = result.trend\n",
    "    daily_quantity_papagrande['FP_seasonal_daily'] = result.seasonal\n",
    "    daily_quantity_papagrande['FP_residual_daily_lag2'] = daily_quantity_papagrande['FP_residual_daily'].shift(2)\n",
    "    daily_quantity_papagrande['FP_trend_daily_lag2'] = daily_quantity_papagrande['FP_trend_daily'].shift(2)\n",
    "    daily_quantity_papagrande['FP_seasonal_daily_lag2'] = daily_quantity_papagrande['FP_seasonal_daily'].shift(2)\n",
    "   \n",
    "    \n",
    "    daily_quantity_papagrande['FP_rolling_avg_quantity_7'] = (\n",
    "        daily_quantity_papagrande['quantity']\n",
    "        .rolling(window=7)\n",
    "        .mean()\n",
    "        .shift(2)  # This adds the 1-day lag\n",
    "    )\n",
    "\n",
    "    daily_quantity_papagrande['FP_rolling_avg_quantity_30'] = (\n",
    "        daily_quantity_papagrande['quantity']\n",
    "        .rolling(window=30)\n",
    "        .mean()\n",
    "        .shift(2)  # This adds the 1-day lag\n",
    "    )\n",
    "\n",
    "    daily_quantity_papagrande['FP_quantity_daily_lag2'] = (\n",
    "            daily_quantity_papagrande['quantity']\n",
    "            .shift(2)  # This adds the 1-day lag\n",
    "        )\n",
    "    daily_quantity_papagrande['FP_net_price_daily_lag2'] = (\n",
    "            daily_quantity_papagrande['net_price']\n",
    "            .shift(2)  # This adds the 1-day lag\n",
    "        )\n",
    "\n",
    "    daily_quantity_papagrande['FP_net_price_daily_lag1'] = (\n",
    "            daily_quantity_papagrande['net_price']\n",
    "            .shift(1)  # This adds the 1-day lag\n",
    "        )\n",
    "\n",
    "    def ewm_features(dataframe, alphas, lags):\n",
    "      for alpha in alphas:\n",
    "          for lag in lags:\n",
    "              dataframe['FP_quantity_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n",
    "                  dataframe['quantity'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n",
    "      dataframe.fillna(0, inplace=True)\n",
    "      return dataframe\n",
    "    \n",
    "    alphas = [0.95, 0.5]\n",
    "    lags   = [2,7,10,14,30,60]\n",
    "\n",
    "\n",
    "    daily_quantity_papagrande= ewm_features(daily_quantity_papagrande, alphas, lags)\n",
    "\n",
    "    daily_quantity_papagrande['FP_daily_net_price'] = daily_quantity_papagrande['net_price']\n",
    "    daily_quantity_papagrande['FP_daily_quantity'] = daily_quantity_papagrande['quantity']\n",
    "    daily_quantity_papagrande['date']= daily_quantity_papagrande['date'].astype(str)\n",
    "    merged_df['date']= merged_df['date'].astype(str)\n",
    "    \n",
    "    return daily_quantity_papagrande, merged_df\n",
    "\n",
    "\n",
    "\n",
    "def aggregate_by_hour(df, time_bucket, has_competitor_price):\n",
    "    \"\"\"\n",
    "    Aggregate a DataFrame by a specified time bucket (e.g., 24 for daily aggregation).\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with a datetime field.\n",
    "        time_bucket (int): Number of hours in each time bucket for aggregation.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated DataFrame with the datetime bucketed and values aggregated.\n",
    "    \"\"\"\n",
    "    # Create a timezone object for Colombia (CST)\n",
    "    colombia_tz = pytz.timezone('America/Bogota')\n",
    "    \n",
    "    # Convert the 'order_submited_datetime' column to Colombia timezone\n",
    "    df['order_submited_datetime'] = df['order_submited_datetime'].dt.tz_convert(colombia_tz)\n",
    "\n",
    "    df['order_submited_datetime'] = pd.to_datetime(df['order_submited_datetime'])\n",
    "    df['hour'] = df['order_submited_datetime'].dt.hour\n",
    "    df['date'] = df['order_submited_datetime'].dt.date\n",
    "    df['month'] = df['order_submited_datetime'].dt.month\n",
    "    # df['day_of_week_n'] = df['order_submited_datetime'].dt.day_of_week\n",
    "    df['day_of_month'] = df['order_submited_datetime'].dt.day\n",
    "    df['year'] = df['order_submited_datetime'].dt.year\n",
    "    agg_df = df.groupby(['date', 'hour', 'net_price']).agg({\n",
    "    'quantity': 'sum',\n",
    "    'cost': 'first',\n",
    "    'avg_bench': 'first',\n",
    "    'product_id': 'first',\n",
    "    'product_name': 'first',\n",
    "    'gross_price': 'first',\n",
    "    # 'day_of_week_n': 'first',\n",
    "    'day_of_month': 'first',\n",
    "    # 'date': 'first'\n",
    "    'year':'first',\n",
    "    'month': 'first'\n",
    "\n",
    "}).reset_index()\n",
    "\n",
    "    min_date = pd.to_datetime(agg_df['date']).min()\n",
    "    max_date = pd.to_datetime(agg_df['date']).max()\n",
    "\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    hour_range = list(range(24))\n",
    "\n",
    "    full_df = pd.DataFrame([(date, hour) for date in date_range for hour in hour_range], columns=['date', 'hour'])\n",
    "    full_df['date'] = full_df['date'].dt.date.astype(str)  # Convert datetime to string to match df's 'date' format\n",
    "    temp = agg_df[['date','hour']]\n",
    "    temp_unique = temp.drop_duplicates()\n",
    "    temp_unique['date'] = pd.to_datetime(temp_unique['date'])\n",
    "    temp_unique['date'] = temp_unique['date'].dt.date.astype(str)\n",
    "    # Identify missing rows\n",
    "    merged_df = pd.merge(full_df, temp_unique, on=['date', 'hour'], how='left', indicator=True)\n",
    "    missing_rows = merged_df[merged_df['_merge'] == 'left_only'][['date', 'hour']]\n",
    "    # missing_rows.head(23)\n",
    "    agg_df['date'] = pd.to_datetime(agg_df['date'])\n",
    "    agg_df['date'] = agg_df['date'].dt.date.astype(str)\n",
    "    merged_df = pd.merge(agg_df, missing_rows, on=['date', 'hour'], how='outer')\n",
    "    new_date = merged_df.iloc[-1]['date']\n",
    "    new_price = merged_df.iloc[-1]['net_price']\n",
    "\n",
    "\n",
    "    date_format = '%Y-%m-%d'\n",
    "    date_DATE = datetime.strptime(new_date, date_format)\n",
    "    # Add one day to the date\n",
    "    next_date = date_DATE + timedelta(days=1)\n",
    "    next_date_str = next_date.strftime(date_format)\n",
    "    print('next date',next_date_str)\n",
    "    test_vector_copy = generate_datafrane_for_profit2(next_date_str,new_price )\n",
    "    print('new_test_vecotr',test_vector_copy.iloc[-1]['date'])\n",
    "    merged_df = merged_df.sort_values(['date','hour'])\n",
    "    merged_df = pd.concat([ merged_df, test_vector_copy])\n",
    "    print('new_row',merged_df.iloc[-1]['date'])\n",
    "\n",
    "    test_vector_copy = generate_datafrane_for_profit2('2023-10-19',new_price )\n",
    "    print('new_test_vecotr',test_vector_copy.iloc[-1]['date'])\n",
    "    merged_df = merged_df.sort_values(['date','hour'])\n",
    "    merged_df = pd.concat([ merged_df, test_vector_copy])\n",
    "    print('new_row',merged_df.iloc[-1]['date'])\n",
    "\n",
    "\n",
    "    cols_to_fill = [col for col in merged_df.columns if col not in ['quantity']]\n",
    "    merged_df = merged_df.sort_values(['date','hour'])\n",
    "    merged_df['net_price'] = merged_df['net_price'].fillna(method='ffill')\n",
    "   \n",
    "    \n",
    "    \n",
    "        \n",
    "    merged_df['quantity'].fillna(0, inplace=True)\n",
    "\n",
    "    # if not comp\n",
    "    daily_quantity = merged_df.groupby('date').agg({'quantity': 'sum', 'net_price': 'mean'}).reset_index()\n",
    "\n",
    "\n",
    "    result = seasonal_decompose(daily_quantity['quantity'], model='additive', period=7)\n",
    "    daily_quantity['residual_daily'] = result.resid\n",
    "    daily_quantity['trend_daily'] = result.trend\n",
    "    daily_quantity['seasonal_daily'] = result.seasonal\n",
    "    daily_quantity['residual_daily_lag2'] = daily_quantity['residual_daily'].shift(2)\n",
    "    daily_quantity['trend_daily_lag2'] = daily_quantity['trend_daily'].shift(2)\n",
    "    daily_quantity['seasonal_daily_lag2'] = daily_quantity['seasonal_daily'].shift(2)\n",
    "   \n",
    "    #  if  comp\n",
    "    # daily_quantity = merged_df.groupby('date').agg({'quantity': 'sum', 'net_price': 'mean','avg_bench':'mean'}).reset_index()\n",
    "\n",
    "    # Step 2: Calculate 7-Day Rolling Average with 1-day Lag\n",
    "    daily_quantity['rolling_avg_quantity_7_lag2'] = (\n",
    "        daily_quantity['quantity']\n",
    "        .rolling(window=7)\n",
    "        .mean()\n",
    "        .shift(2)  # This adds the 1-day lag\n",
    "    )\n",
    "\n",
    "    daily_quantity['rolling_avg_quantity_10_lag2'] = (\n",
    "        daily_quantity['quantity']\n",
    "        .rolling(window=10)\n",
    "        .mean()\n",
    "        .shift(2)  # This adds the 1-day lag\n",
    "    )\n",
    "\n",
    "    daily_quantity['rolling_avg_quantity_15_lag2'] = (\n",
    "        daily_quantity['quantity']\n",
    "        .rolling(window=15)\n",
    "        .mean()\n",
    "        .shift(2)  # This adds the 1-day lag\n",
    "    )\n",
    "\n",
    "    daily_quantity['rolling_avg_quantity_30_lag2'] = (\n",
    "        daily_quantity['quantity']\n",
    "        .rolling(window=30)\n",
    "        .mean()\n",
    "        .shift(2)  # This adds the 1-day lag\n",
    "    )\n",
    "\n",
    "    daily_quantity['quantity_daily_lag2'] = (\n",
    "        daily_quantity['quantity']\n",
    "        .shift(2)  # This adds the 1-day lag\n",
    "    )\n",
    "\n",
    "    daily_quantity['ratio_rolling_avg_quantity_30_7_lag2'] = daily_quantity['rolling_avg_quantity_30_lag2']/daily_quantity['rolling_avg_quantity_7_lag2']\n",
    "    \n",
    "\n",
    "    def ewm_features(dataframe, alphas, lags):\n",
    "      for alpha in alphas:\n",
    "          for lag in lags:\n",
    "              dataframe['quantity_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n",
    "                  dataframe['quantity'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n",
    "      dataframe.fillna(0, inplace=True)\n",
    "      return dataframe\n",
    "    \n",
    "    alphas = [0.95, 0.5]\n",
    "    lags   = [2,7,10,14,30,60]\n",
    "\n",
    "\n",
    "    daily_quantity= ewm_features(daily_quantity, alphas, lags)\n",
    "\n",
    "\n",
    "    # daily_quantity['diff'] = daily_quantity['net_price'] - daily_quantity['avg_bench']\n",
    "    # daily_quantity['avg_bench_lag1'] = daily_quantity['avg_bench'].shift(1)\n",
    "    # daily_quantity['diff_cross'] = daily_quantity['net_price'] - daily_quantity['avg_bench_lag1']\n",
    "\n",
    "    # daily_quantity['diff_lag2'] = (\n",
    "    #     daily_quantity['diff']\n",
    "    #     .shift(2)\n",
    "    #  # This adds the 1-day lag\n",
    "    # )\n",
    "    # daily_quantity['diff_cross_lag2'] = (\n",
    "    #     daily_quantity['diff_cross']\n",
    "    #     .shift(2)\n",
    "    #  # This adds the 1-day lag\n",
    "    # )\n",
    "\n",
    "    # merged_df = merged_df.merge(daily_quantity[['date', 'rolling_avg_quantity_30','rolling_avg_quantity_7','quantity_daily_lag2']], on='date', how='left')\n",
    "    \n",
    "    merged_df = merged_df.merge(daily_quantity[['date', 'ratio_rolling_avg_quantity_30_7_lag2','quantity_ewm_alpha_095_lag_2', 'quantity_ewm_alpha_095_lag_7','quantity_ewm_alpha_095_lag_10','quantity_ewm_alpha_095_lag_14'\n",
    "                                                ,'quantity_ewm_alpha_095_lag_30','quantity_ewm_alpha_095_lag_60','quantity_ewm_alpha_05_lag_2', 'quantity_ewm_alpha_05_lag_7','quantity_ewm_alpha_05_lag_10','quantity_ewm_alpha_05_lag_14'\n",
    "                                                ,'quantity_ewm_alpha_05_lag_30','quantity_ewm_alpha_05_lag_60',\n",
    "                                                'rolling_avg_quantity_30_lag2','rolling_avg_quantity_15_lag2','rolling_avg_quantity_10_lag2','rolling_avg_quantity_7_lag2','quantity_daily_lag2','trend_daily_lag2','seasonal_daily_lag2','residual_daily_lag2']], on='date', how='left')\n",
    "    \n",
    "    daily_quantity_FP, hourly_FP = get_papa_grande()\n",
    "    merged_df = merged_df.merge(daily_quantity_FP[['date','FP_quantity_ewm_alpha_095_lag_7','FP_trend_daily_lag2','FP_net_price_daily_lag1','FP_net_price_daily_lag2', 'FP_quantity_daily_lag2','FP_rolling_avg_quantity_30','FP_rolling_avg_quantity_7','FP_daily_quantity','FP_daily_net_price']], on='date', how='left')\n",
    "\n",
    "    merged_df = merged_df.merge(hourly_FP[['date','hour','net_price_FP_lag48','quantity_FP_lag48','net_price_FP_lag49','quantity_FP_lag49','net_price_FP','quantity_FP']], on=['date', 'hour'], how='left')\n",
    "\n",
    "    merged_df = merged_df.sort_values(['date','hour'])\n",
    "    merged_df['date']=merged_df['date'].astype(str)\n",
    "    \n",
    "    merged_df = merged_df.sort_values(['date','hour'])\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "def create_features(dataframe, has_competitor_price):\n",
    "    def is_weekend(day):\n",
    "        return 1 if day in [5, 6] else 0\n",
    "    \n",
    "    # dataframe['datetime'] = pd.to_datetime(dataframe['date'])\n",
    "    \n",
    "    cols_to_fill = [col for col in dataframe.columns if col not in ['quantity']]\n",
    "    dataframe = dataframe.sort_values(['date','hour'])\n",
    "    # dataframe['net_price'] = dataframe['net_price'].fillna(method='ffill')\n",
    "    dataframe['quantity'].fillna(0, inplace=True)\n",
    "\n",
    "    dataframe = add_inflation(dataframe)\n",
    "    dataframe['day_of_week_n'] = dataframe['date'].dt.day_of_week\n",
    "\n",
    "    dataframe['if_weekend'] = dataframe['day_of_week_n'].apply(is_weekend)\n",
    "    for day_num in range(0, 7):\n",
    "        day_col_name = f'net_price_day_{day_num}'\n",
    "        dataframe[day_col_name] = dataframe.apply(lambda row: row['net_price'] if row['day_of_week_n'] == day_num else 0, axis=1)\n",
    "\n",
    "    print('features ', dataframe.info())\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kq/5tmn8y9n1fzgmrhxm2d96s700000gn/T/ipykernel_10583/757281885.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered['order_submited_datetime'] = pd.to_datetime(filtered['order_submited_datetime'])\n",
      "/var/folders/kq/5tmn8y9n1fzgmrhxm2d96s700000gn/T/ipykernel_10583/757281885.py:315: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_unique['date'] = pd.to_datetime(temp_unique['date'])\n",
      "/var/folders/kq/5tmn8y9n1fzgmrhxm2d96s700000gn/T/ipykernel_10583/757281885.py:316: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp_unique['date'] = temp_unique['date'].dt.date.astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next date 2023-10-18\n",
      "new_test_vecotr 2023-10-18\n",
      "new_row 2023-10-18\n",
      "new_test_vecotr 2023-10-19\n",
      "new_row 2023-10-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kq/5tmn8y9n1fzgmrhxm2d96s700000gn/T/ipykernel_10583/757281885.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered['order_submited_datetime'] = pd.to_datetime(filtered['order_submited_datetime'])\n",
      "/var/folders/kq/5tmn8y9n1fzgmrhxm2d96s700000gn/T/ipykernel_10583/757281885.py:80: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  return stats.mode(x)[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next date 2023-10-18\n",
      "new_test_vecotr 2023-10-18\n",
      "new_test_vecotr 2023-10-19\n",
      "new_row 2023-10-19\n",
      "new_row 2023-10-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kq/5tmn8y9n1fzgmrhxm2d96s700000gn/T/ipykernel_10583/757281885.py:80: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  return stats.mode(x)[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18186 entries, 0 to 18185\n",
      "Data columns (total 59 columns):\n",
      " #   Column                                Non-Null Count  Dtype         \n",
      "---  ------                                --------------  -----         \n",
      " 0   date                                  18186 non-null  datetime64[ns]\n",
      " 1   hour                                  18186 non-null  int64         \n",
      " 2   net_price                             18177 non-null  float64       \n",
      " 3   quantity                              18186 non-null  float64       \n",
      " 4   cost                                  6068 non-null   float64       \n",
      " 5   avg_bench                             6529 non-null   float64       \n",
      " 6   product_id                            6557 non-null   float64       \n",
      " 7   product_name                          6557 non-null   object        \n",
      " 8   gross_price                           6557 non-null   float64       \n",
      " 9   day_of_month                          6557 non-null   float64       \n",
      " 10  year                                  6557 non-null   float64       \n",
      " 11  month                                 6557 non-null   float64       \n",
      " 12  datetime                              48 non-null     datetime64[ns]\n",
      " 13  day_of_week_n                         18186 non-null  int32         \n",
      " 14  net_price_day_0                       18177 non-null  float64       \n",
      " 15  net_price_day_1                       18186 non-null  float64       \n",
      " 16  net_price_day_2                       18186 non-null  float64       \n",
      " 17  net_price_day_3                       18186 non-null  float64       \n",
      " 18  net_price_day_4                       18186 non-null  float64       \n",
      " 19  net_price_day_5                       18186 non-null  float64       \n",
      " 20  net_price_day_6                       18186 non-null  float64       \n",
      " 21  ratio_rolling_avg_quantity_30_7_lag2  18186 non-null  float64       \n",
      " 22  quantity_ewm_alpha_095_lag_2          18186 non-null  float64       \n",
      " 23  quantity_ewm_alpha_095_lag_7          18186 non-null  float64       \n",
      " 24  quantity_ewm_alpha_095_lag_10         18186 non-null  float64       \n",
      " 25  quantity_ewm_alpha_095_lag_14         18186 non-null  float64       \n",
      " 26  quantity_ewm_alpha_095_lag_30         18186 non-null  float64       \n",
      " 27  quantity_ewm_alpha_095_lag_60         18186 non-null  float64       \n",
      " 28  quantity_ewm_alpha_05_lag_2           18186 non-null  float64       \n",
      " 29  quantity_ewm_alpha_05_lag_7           18186 non-null  float64       \n",
      " 30  quantity_ewm_alpha_05_lag_10          18186 non-null  float64       \n",
      " 31  quantity_ewm_alpha_05_lag_14          18186 non-null  float64       \n",
      " 32  quantity_ewm_alpha_05_lag_30          18186 non-null  float64       \n",
      " 33  quantity_ewm_alpha_05_lag_60          18186 non-null  float64       \n",
      " 34  rolling_avg_quantity_30_lag2          18186 non-null  float64       \n",
      " 35  rolling_avg_quantity_15_lag2          18186 non-null  float64       \n",
      " 36  rolling_avg_quantity_10_lag2          18186 non-null  float64       \n",
      " 37  rolling_avg_quantity_7_lag2           18186 non-null  float64       \n",
      " 38  quantity_daily_lag2                   18186 non-null  float64       \n",
      " 39  trend_daily_lag2                      18186 non-null  float64       \n",
      " 40  seasonal_daily_lag2                   18186 non-null  float64       \n",
      " 41  residual_daily_lag2                   18186 non-null  float64       \n",
      " 42  FP_quantity_ewm_alpha_095_lag_7       18186 non-null  float64       \n",
      " 43  FP_trend_daily_lag2                   18186 non-null  float64       \n",
      " 44  FP_net_price_daily_lag1               18186 non-null  float64       \n",
      " 45  FP_net_price_daily_lag2               18186 non-null  float64       \n",
      " 46  FP_quantity_daily_lag2                18186 non-null  float64       \n",
      " 47  FP_rolling_avg_quantity_30            18186 non-null  float64       \n",
      " 48  FP_rolling_avg_quantity_7             18186 non-null  float64       \n",
      " 49  FP_daily_quantity                     18186 non-null  float64       \n",
      " 50  FP_daily_net_price                    18186 non-null  float64       \n",
      " 51  net_price_FP_lag48                    18122 non-null  float64       \n",
      " 52  quantity_FP_lag48                     18133 non-null  float64       \n",
      " 53  net_price_FP_lag49                    18121 non-null  float64       \n",
      " 54  quantity_FP_lag49                     18132 non-null  float64       \n",
      " 55  net_price_FP                          18175 non-null  float64       \n",
      " 56  quantity_FP                           18186 non-null  float64       \n",
      " 57  inflation                             18186 non-null  float64       \n",
      " 58  if_weekend                            18186 non-null  int64         \n",
      "dtypes: datetime64[ns](2), float64(53), int32(1), int64(2), object(1)\n",
      "memory usage: 8.1+ MB\n",
      "features  None\n"
     ]
    }
   ],
   "source": [
    "time_bucket=24\n",
    "has_competitor_price=1\n",
    "\n",
    "filtered1 = get_filterd_dataset('Champiñón Blanco Paris 1Kg Bandeja')\n",
    "\n",
    "daily_aggregated_df = aggregate_by_hour(filtered1, time_bucket=time_bucket, has_competitor_price=has_competitor_price)\n",
    "features = create_features(daily_aggregated_df, has_competitor_price)\n",
    "\n",
    "features = features.sort_values(['date','hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18186 entries, 0 to 18185\n",
      "Data columns (total 15 columns):\n",
      " #   Column                        Non-Null Count  Dtype         \n",
      "---  ------                        --------------  -----         \n",
      " 0   date                          18186 non-null  datetime64[ns]\n",
      " 1   net_price                     18177 non-null  float64       \n",
      " 2   day_of_week_n                 18186 non-null  int32         \n",
      " 3   net_price_day_0               18177 non-null  float64       \n",
      " 4   net_price_day_1               18186 non-null  float64       \n",
      " 5   net_price_day_2               18186 non-null  float64       \n",
      " 6   net_price_day_3               18186 non-null  float64       \n",
      " 7   net_price_day_4               18186 non-null  float64       \n",
      " 8   net_price_day_5               18186 non-null  float64       \n",
      " 9   net_price_day_6               18186 non-null  float64       \n",
      " 10  hour                          18186 non-null  int64         \n",
      " 11  quantity_ewm_alpha_095_lag_7  18186 non-null  float64       \n",
      " 12  FP_quantity_daily_lag2        18186 non-null  float64       \n",
      " 13  trend_daily_lag2              18186 non-null  float64       \n",
      " 14  quantity                      18186 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(12), int32(1), int64(1)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "features[[\n",
    "    'date', 'net_price', 'day_of_week_n', 'net_price_day_0', 'net_price_day_1',\n",
    "    'net_price_day_2', 'net_price_day_3', 'net_price_day_4', 'net_price_day_5',\n",
    "    'net_price_day_6', 'hour', 'quantity_ewm_alpha_095_lag_7',\n",
    "    'FP_quantity_daily_lag2', 'trend_daily_lag2', 'quantity'\n",
    "]].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv('combined_data/CBP1kg_500(mode).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kq/5tmn8y9n1fzgmrhxm2d96s700000gn/T/ipykernel_10583/1689469489.py:1: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pd.read_csv('combined_data/PapaSabaneraSM_GG(mode).csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>net_price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>cost</th>\n",
       "      <th>avg_bench</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>gross_price</th>\n",
       "      <th>...</th>\n",
       "      <th>FP_daily_quantity</th>\n",
       "      <th>FP_daily_net_price</th>\n",
       "      <th>net_price_FP_lag48</th>\n",
       "      <th>quantity_FP_lag48</th>\n",
       "      <th>net_price_FP_lag49</th>\n",
       "      <th>quantity_FP_lag49</th>\n",
       "      <th>net_price_FP</th>\n",
       "      <th>quantity_FP</th>\n",
       "      <th>inflation</th>\n",
       "      <th>if_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-10-17</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-10-17</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-10-17</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-10-17</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-10-17</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17683</th>\n",
       "      <td>17683</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>19</td>\n",
       "      <td>5000.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17684</th>\n",
       "      <td>17684</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>20</td>\n",
       "      <td>5000.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17685</th>\n",
       "      <td>17685</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>21</td>\n",
       "      <td>5000.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17686</th>\n",
       "      <td>17686</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>22</td>\n",
       "      <td>5000.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17687</th>\n",
       "      <td>17687</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>23</td>\n",
       "      <td>5000.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17688 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0        date  hour  net_price  quantity  cost  avg_bench  \\\n",
       "0               0  2021-10-17     0        NaN       0.0   NaN        NaN   \n",
       "1               1  2021-10-17     1        NaN       0.0   NaN        NaN   \n",
       "2               2  2021-10-17     2        NaN       0.0   NaN        NaN   \n",
       "3               3  2021-10-17     3        NaN       0.0   NaN        NaN   \n",
       "4               4  2021-10-17     4        NaN       0.0   NaN        NaN   \n",
       "...           ...         ...   ...        ...       ...   ...        ...   \n",
       "17683       17683  2023-10-19    19     5000.8       0.0   NaN        NaN   \n",
       "17684       17684  2023-10-19    20     5000.8       0.0   NaN        NaN   \n",
       "17685       17685  2023-10-19    21     5000.8       0.0   NaN        NaN   \n",
       "17686       17686  2023-10-19    22     5000.8       0.0   NaN        NaN   \n",
       "17687       17687  2023-10-19    23     5000.8       0.0   NaN        NaN   \n",
       "\n",
       "       product_id product_name  gross_price  ...  FP_daily_quantity  \\\n",
       "0             NaN          NaN          NaN  ...                7.5   \n",
       "1             NaN          NaN          NaN  ...                7.5   \n",
       "2             NaN          NaN          NaN  ...                7.5   \n",
       "3             NaN          NaN          NaN  ...                7.5   \n",
       "4             NaN          NaN          NaN  ...                7.5   \n",
       "...           ...          ...          ...  ...                ...   \n",
       "17683         NaN          NaN          NaN  ...                0.0   \n",
       "17684         NaN          NaN          NaN  ...                0.0   \n",
       "17685         NaN          NaN          NaN  ...                0.0   \n",
       "17686         NaN          NaN          NaN  ...                0.0   \n",
       "17687         NaN          NaN          NaN  ...                0.0   \n",
       "\n",
       "       FP_daily_net_price  net_price_FP_lag48 quantity_FP_lag48  \\\n",
       "0                  3450.0                 NaN               NaN   \n",
       "1                  3450.0                 NaN               NaN   \n",
       "2                  3450.0                 NaN               NaN   \n",
       "3                  3450.0                 NaN               NaN   \n",
       "4                  3450.0                 NaN               NaN   \n",
       "...                   ...                 ...               ...   \n",
       "17683              5400.0              5400.0              17.5   \n",
       "17684              5400.0              5400.0               0.0   \n",
       "17685              5400.0              5400.0               0.0   \n",
       "17686              5400.0              5400.0               0.0   \n",
       "17687              5400.0              5400.0               0.0   \n",
       "\n",
       "       net_price_FP_lag49  quantity_FP_lag49  net_price_FP  quantity_FP  \\\n",
       "0                     NaN                NaN        3450.0          0.0   \n",
       "1                     NaN                NaN        3450.0          0.0   \n",
       "2                     NaN                NaN        3450.0          0.0   \n",
       "3                     NaN                NaN        3450.0          0.0   \n",
       "4                     NaN                NaN        3450.0          0.0   \n",
       "...                   ...                ...           ...          ...   \n",
       "17683              5400.0               12.5        5400.0          0.0   \n",
       "17684              5400.0               17.5        5400.0          0.0   \n",
       "17685              5400.0                0.0        5400.0          0.0   \n",
       "17686              5400.0                0.0        5400.0          0.0   \n",
       "17687              5400.0                0.0        5400.0          0.0   \n",
       "\n",
       "       inflation  if_weekend  \n",
       "0           4.58           1  \n",
       "1           4.58           1  \n",
       "2           4.58           1  \n",
       "3           4.58           1  \n",
       "4           4.58           1  \n",
       "...          ...         ...  \n",
       "17683      10.79           0  \n",
       "17684      10.79           0  \n",
       "17685      10.79           0  \n",
       "17686      10.79           0  \n",
       "17687      10.79           0  \n",
       "\n",
       "[17688 rows x 58 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('combined_data/PapaSabaneraSM_PCSM(mode).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Master Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_16 = pd.read_csv('data/Frubana_2023_10_16.csv')\n",
    "# colombia_tz = pytz.timezone('America/Bogota')\n",
    " \n",
    "# df_16['order_submited_datetime'] = pd.to_datetime(df_16['order_submited_datetime'])\n",
    "# df_16['order_submited_datetime'] = df_16['order_submited_datetime'].dt.tz_localize(pytz.utc).dt.tz_convert(colombia_tz)\n",
    "# df_16['order_submited_date'] = df_16['order_submited_datetime'].dt.date.astype(str)\n",
    "# df_16 = df_16[df_16['order_submited_date']<'2023-10-16']\n",
    "# df_16.to_csv('data/master/master_historical.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataframes\n",
    "df_17 = pd.read_csv('data/Frubana_2023_10_18.csv')\n",
    "df_16 = pd.read_csv('data/master/master_historical_till_2023_10_16.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the first few rows of each dataframe\n",
    "df_17_head = df_17.head()\n",
    "\n",
    "colombia_tz = pytz.timezone('America/Bogota')\n",
    "    # Convert the 'order_submited_datetime' column to Colombia timezone\n",
    "df_17['order_submited_datetime'] = pd.to_datetime(df_17['order_submited_datetime'])\n",
    "df_17['order_submited_datetime'] = df_17['order_submited_datetime'].dt.tz_localize(pytz.utc).dt.tz_convert(colombia_tz)\n",
    "df_17['order_submited_date'] = df_17['order_submited_datetime'].dt.date.astype(str)\n",
    "df_17 = df_17[df_17['order_submited_date']<'2023-10-18']\n",
    "df_17 = df_17[df_17['order_submited_date']>'2023-10-16']\n",
    "\n",
    "combined_df = pd.concat([df_17, df_16], ignore_index=True)\n",
    "combined_df.to_csv('data/master/master_historical_till_2023_10_17.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>region_code</th>\n",
       "      <th>order_submited_date</th>\n",
       "      <th>order_submited_datetime</th>\n",
       "      <th>parent_category</th>\n",
       "      <th>category</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>cost</th>\n",
       "      <th>benchmark</th>\n",
       "      <th>median_bench</th>\n",
       "      <th>avg_bench</th>\n",
       "      <th>max_bench</th>\n",
       "      <th>gross_price</th>\n",
       "      <th>net_price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>product_step_unit</th>\n",
       "      <th>benchmark_count</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22826</th>\n",
       "      <td>22826</td>\n",
       "      <td>BOG</td>\n",
       "      <td>2021-10-16</td>\n",
       "      <td>2021-10-16 19:04:00-05:00</td>\n",
       "      <td>Frutas &amp; verduras</td>\n",
       "      <td>Verduras</td>\n",
       "      <td>-882794291467029550</td>\n",
       "      <td>Pimentón Maduración Mixta Semi (Mediano / Nata...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2822.00</td>\n",
       "      <td>2116.50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22015.0</td>\n",
       "      <td>21291.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215856</th>\n",
       "      <td>215856</td>\n",
       "      <td>BOG</td>\n",
       "      <td>2021-10-16</td>\n",
       "      <td>2021-10-16 19:06:00-05:00</td>\n",
       "      <td>Frutas &amp; verduras</td>\n",
       "      <td>Tubérculos</td>\n",
       "      <td>8673176760259137473</td>\n",
       "      <td>Papa Sabanera Gruesa (Grande) Kg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3450.00</td>\n",
       "      <td>3450.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215045.0</td>\n",
       "      <td>214378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313146</th>\n",
       "      <td>313146</td>\n",
       "      <td>BOG</td>\n",
       "      <td>2021-10-16</td>\n",
       "      <td>2021-10-16 19:24:00-05:00</td>\n",
       "      <td>Frutas &amp; verduras</td>\n",
       "      <td>Verduras</td>\n",
       "      <td>-882794291467029550</td>\n",
       "      <td>Pimentón Maduración Mixta Semi (Mediano / Nata...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2822.00</td>\n",
       "      <td>2116.50</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>312335.0</td>\n",
       "      <td>311708.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94578</th>\n",
       "      <td>94578</td>\n",
       "      <td>BOG</td>\n",
       "      <td>2021-10-16</td>\n",
       "      <td>2021-10-16 19:45:00-05:00</td>\n",
       "      <td>Frutas &amp; verduras</td>\n",
       "      <td>Verduras</td>\n",
       "      <td>-882794291467029550</td>\n",
       "      <td>Pimentón Maduración Mixta Semi (Mediano / Nata...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2822.00</td>\n",
       "      <td>2116.50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93767.0</td>\n",
       "      <td>93061.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22820</th>\n",
       "      <td>22820</td>\n",
       "      <td>BOG</td>\n",
       "      <td>2021-10-16</td>\n",
       "      <td>2021-10-16 19:48:00-05:00</td>\n",
       "      <td>Frutas &amp; verduras</td>\n",
       "      <td>Tubérculos</td>\n",
       "      <td>-3386165639547488916</td>\n",
       "      <td>Papa Criolla Lavada Semi (Mediana) Kg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3920.00</td>\n",
       "      <td>3920.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22009.0</td>\n",
       "      <td>21285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>115</td>\n",
       "      <td>BOG</td>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>2023-10-17 23:06:00-05:00</td>\n",
       "      <td>Frutas &amp; verduras</td>\n",
       "      <td>Verduras</td>\n",
       "      <td>-6536998881607504388</td>\n",
       "      <td>Apio Estándar (Grande) Atado</td>\n",
       "      <td>615.0477</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.00</td>\n",
       "      <td>1034.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>BOG</td>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>2023-10-17 23:10:00-05:00</td>\n",
       "      <td>Frutas &amp; verduras</td>\n",
       "      <td>Tubérculos</td>\n",
       "      <td>-8910485076695139605</td>\n",
       "      <td>Papa Criolla Segunda Mixta Kg</td>\n",
       "      <td>1614.5149</td>\n",
       "      <td>2418.0</td>\n",
       "      <td>2418.0</td>\n",
       "      <td>2418.0</td>\n",
       "      <td>2418.0</td>\n",
       "      <td>2680.00</td>\n",
       "      <td>2519.20</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>244</td>\n",
       "      <td>BOG</td>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>2023-10-17 23:10:00-05:00</td>\n",
       "      <td>Frutas &amp; verduras</td>\n",
       "      <td>Verduras</td>\n",
       "      <td>-882794291467029550</td>\n",
       "      <td>Pimentón Maduración Mixta Semi (Mediano / Nata...</td>\n",
       "      <td>1821.8443</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>3730.00</td>\n",
       "      <td>3730.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>665</td>\n",
       "      <td>BOG</td>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>2023-10-17 23:17:00-05:00</td>\n",
       "      <td>Frutas &amp; verduras</td>\n",
       "      <td>Verduras</td>\n",
       "      <td>7749572820280257375</td>\n",
       "      <td>Cebolla Cabezona Roja Sin Pelar Tamaño Mixto D...</td>\n",
       "      <td>4345.9175</td>\n",
       "      <td>2292.0</td>\n",
       "      <td>2292.0</td>\n",
       "      <td>2292.0</td>\n",
       "      <td>2292.0</td>\n",
       "      <td>2639.92</td>\n",
       "      <td>2481.52</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>BOG</td>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>2023-10-17 23:42:00-05:00</td>\n",
       "      <td>Frutas &amp; verduras</td>\n",
       "      <td>Verduras</td>\n",
       "      <td>-882794291467029550</td>\n",
       "      <td>Pimentón Maduración Mixta Semi (Mediano / Nata...</td>\n",
       "      <td>1821.8443</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>3730.00</td>\n",
       "      <td>3730.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>325828 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.2 region_code order_submited_date  \\\n",
       "22826          22826         BOG          2021-10-16   \n",
       "215856        215856         BOG          2021-10-16   \n",
       "313146        313146         BOG          2021-10-16   \n",
       "94578          94578         BOG          2021-10-16   \n",
       "22820          22820         BOG          2021-10-16   \n",
       "...              ...         ...                 ...   \n",
       "115              115         BOG          2023-10-17   \n",
       "23                23         BOG          2023-10-17   \n",
       "244              244         BOG          2023-10-17   \n",
       "665              665         BOG          2023-10-17   \n",
       "195              195         BOG          2023-10-17   \n",
       "\n",
       "          order_submited_datetime    parent_category    category  \\\n",
       "22826   2021-10-16 19:04:00-05:00  Frutas & verduras    Verduras   \n",
       "215856  2021-10-16 19:06:00-05:00  Frutas & verduras  Tubérculos   \n",
       "313146  2021-10-16 19:24:00-05:00  Frutas & verduras    Verduras   \n",
       "94578   2021-10-16 19:45:00-05:00  Frutas & verduras    Verduras   \n",
       "22820   2021-10-16 19:48:00-05:00  Frutas & verduras  Tubérculos   \n",
       "...                           ...                ...         ...   \n",
       "115     2023-10-17 23:06:00-05:00  Frutas & verduras    Verduras   \n",
       "23      2023-10-17 23:10:00-05:00  Frutas & verduras  Tubérculos   \n",
       "244     2023-10-17 23:10:00-05:00  Frutas & verduras    Verduras   \n",
       "665     2023-10-17 23:17:00-05:00  Frutas & verduras    Verduras   \n",
       "195     2023-10-17 23:42:00-05:00  Frutas & verduras    Verduras   \n",
       "\n",
       "                 product_id  \\\n",
       "22826   -882794291467029550   \n",
       "215856  8673176760259137473   \n",
       "313146  -882794291467029550   \n",
       "94578   -882794291467029550   \n",
       "22820  -3386165639547488916   \n",
       "...                     ...   \n",
       "115    -6536998881607504388   \n",
       "23     -8910485076695139605   \n",
       "244     -882794291467029550   \n",
       "665     7749572820280257375   \n",
       "195     -882794291467029550   \n",
       "\n",
       "                                             product_name       cost  \\\n",
       "22826   Pimentón Maduración Mixta Semi (Mediano / Nata...        NaN   \n",
       "215856                   Papa Sabanera Gruesa (Grande) Kg        NaN   \n",
       "313146  Pimentón Maduración Mixta Semi (Mediano / Nata...        NaN   \n",
       "94578   Pimentón Maduración Mixta Semi (Mediano / Nata...        NaN   \n",
       "22820               Papa Criolla Lavada Semi (Mediana) Kg        NaN   \n",
       "...                                                   ...        ...   \n",
       "115                          Apio Estándar (Grande) Atado   615.0477   \n",
       "23                          Papa Criolla Segunda Mixta Kg  1614.5149   \n",
       "244     Pimentón Maduración Mixta Semi (Mediano / Nata...  1821.8443   \n",
       "665     Cebolla Cabezona Roja Sin Pelar Tamaño Mixto D...  4345.9175   \n",
       "195     Pimentón Maduración Mixta Semi (Mediano / Nata...  1821.8443   \n",
       "\n",
       "        benchmark  median_bench  avg_bench  max_bench  gross_price  net_price  \\\n",
       "22826         NaN           NaN        NaN        NaN      2822.00    2116.50   \n",
       "215856        NaN           NaN        NaN        NaN      3450.00    3450.00   \n",
       "313146        NaN           NaN        NaN        NaN      2822.00    2116.50   \n",
       "94578         NaN           NaN        NaN        NaN      2822.00    2116.50   \n",
       "22820         NaN           NaN        NaN        NaN      3920.00    3920.00   \n",
       "...           ...           ...        ...        ...          ...        ...   \n",
       "115        1200.0        1200.0     1200.0     1200.0      1100.00    1034.00   \n",
       "23         2418.0        2418.0     2418.0     2418.0      2680.00    2519.20   \n",
       "244        4000.0        4000.0     4000.0     4000.0      3730.00    3730.00   \n",
       "665        2292.0        2292.0     2292.0     2292.0      2639.92    2481.52   \n",
       "195        4000.0        4000.0     4000.0     4000.0      3730.00    3730.00   \n",
       "\n",
       "        quantity  product_step_unit  benchmark_count  Unnamed: 0.1  Unnamed: 0  \n",
       "22826        1.5                1.5              NaN       22015.0     21291.0  \n",
       "215856       2.5                2.5              NaN      215045.0    214378.0  \n",
       "313146       1.5                1.5              NaN      312335.0    311708.0  \n",
       "94578        3.0                1.5              NaN       93767.0     93061.0  \n",
       "22820        4.0                1.0              NaN       22009.0     21285.0  \n",
       "...          ...                ...              ...           ...         ...  \n",
       "115         15.0                5.0              1.0           NaN         NaN  \n",
       "23          10.0               10.0              1.0           NaN         NaN  \n",
       "244          2.0                1.0              1.0           NaN         NaN  \n",
       "665          3.0                1.0              2.0           NaN         NaN  \n",
       "195          2.0                1.0              1.0           NaN         NaN  \n",
       "\n",
       "[325828 rows x 20 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/master/master_historical_till_2023_10_17_temp.csv')\n",
    "data.sort_values('order_submited_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oxxo_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
